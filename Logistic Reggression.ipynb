{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"ChurnPrediction\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---------------+--------+---------+-------------+-------+--------+-------------+----------+--------+\n",
      "|Churn|AccountWeeks|ContractRenewal|DataPlan|DataUsage|CustServCalls|DayMins|DayCalls|MonthlyCharge|OverageFee|RoamMins|\n",
      "+-----+------------+---------------+--------+---------+-------------+-------+--------+-------------+----------+--------+\n",
      "|    0|         128|              1|       1|      2.7|            1|  265.1|     110|         89.0|      9.87|    10.0|\n",
      "|    0|         107|              1|       1|      3.7|            1|  161.6|     123|         82.0|      9.78|    13.7|\n",
      "|    0|         137|              1|       0|      0.0|            0|  243.4|     114|         52.0|      6.06|    12.2|\n",
      "|    0|          84|              0|       0|      0.0|            2|  299.4|      71|         57.0|       3.1|     6.6|\n",
      "|    0|          75|              0|       0|      0.0|            3|  166.7|     113|         41.0|      7.42|    10.1|\n",
      "+-----+------------+---------------+--------+---------+-------------+-------+--------+-------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load CSV data into Spark DataFrame\n",
    "data = spark.read.csv(r'C:/Users/Tarun Akash/Desktop/df_clean.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[128.0,1.0,1.0,2....|  0.0|\n",
      "|[107.0,1.0,1.0,3....|  0.0|\n",
      "|[137.0,1.0,0.0,0....|  0.0|\n",
      "|[84.0,0.0,0.0,0.0...|  0.0|\n",
      "|[75.0,0.0,0.0,0.0...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "\n",
    "# Convert all columns except 'Churn' into a single feature vector\n",
    "assembler = VectorAssembler(inputCols=[col for col in data.columns if col != 'Churn'], outputCol='features')\n",
    "data = assembler.transform(data)\n",
    "\n",
    "# Convert 'Churn' into a numerical label (if it's categorical)\n",
    "indexer = StringIndexer(inputCol='Churn', outputCol='label')\n",
    "data = indexer.fit(data).transform(data)\n",
    "\n",
    "# Select only the 'features' and 'label' columns for Logistic Regression\n",
    "final_data = data.select('features', 'label')\n",
    "\n",
    "# Show the prepared data\n",
    "final_data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (70% training, 30% testing)\n",
    "train_data, test_data = final_data.randomSplit([0.7, 0.3], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "lr_model = lr.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|[1.0,1.0,0.0,0.0,...|  0.0|       0.0|\n",
      "|[1.0,1.0,1.0,2.27...|  0.0|       0.0|\n",
      "|[2.0,0.0,0.0,0.27...|  1.0|       0.0|\n",
      "|[3.0,0.0,0.0,0.26...|  0.0|       0.0|\n",
      "|[3.0,1.0,1.0,3.21...|  0.0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Show the predictions\n",
    "predictions.select(\"features\", \"label\", \"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8603174603174604\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|   24|\n",
      "|  0.0|       1.0|   20|\n",
      "|  1.0|       0.0|  112|\n",
      "|  0.0|       0.0|  789|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Initialize the evaluator with 'accuracy' metric\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Calculate accuracy on the test data\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Optionally, show the confusion matrix\n",
    "predictions.groupBy(\"label\", \"prediction\").count().show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
